import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path
import wandb
import csv


class Visuals:
    def __init__(self, config, model):
        self.config = config
        self.model = model

    def count_parameters(self):
        """Count total and trainable parameters in the model."""
        if self.model is None:
            raise ValueError("Model not loaded. Call load_model() first.")
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        return total_params, trainable_params

    def plot_trainable_parameters(self, total_params, trainable_params):
        """Plot comparison of total vs trainable parameters."""
        if isinstance(total_params, dict):
            models = list(total_params.keys())
            totals = np.array([total_params[m] for m in models], dtype=float)
            if isinstance(trainable_params, dict):
                trains = np.array([trainable_params.get(m, 0) for m in models], dtype=float)
            else:
                trains = np.array(trainable_params, dtype=float)
        else:
            totals = np.atleast_1d(np.array(total_params, dtype=float))
            trains = np.atleast_1d(np.array(trainable_params, dtype=float))
            models = [str(i) for i in range(len(totals))]

        scale = 1e6
        totals_M = totals / scale
        trains_M = trains / scale

        fig, ax = plt.subplots(figsize=(10, 6))
        x = np.arange(len(models))
        width = 0.36

        bars_total = ax.bar(x - width/2, totals_M, width, label='Total params', color='#3498db')
        bars_train = ax.bar(x + width/2, trains_M, width, label='Trainable params', color='#2ecc71')

        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=30, ha='right')
        ax.set_ylabel('Parameters (M)', fontsize=12)
        ax.set_title('Model Complexity Comparison', fontsize=14, fontweight='bold')
        ax.legend()

        def annotate(bars, vals):
            max_h = max(np.max(totals_M) if len(totals_M) > 0 else 0, 
                       np.max(trains_M) if len(trains_M) > 0 else 0)
            offset = max_h * 0.02 if max_h > 0 else 0.01
            for bar, val in zip(bars, vals):
                ax.text(bar.get_x() + bar.get_width()/2,
                       bar.get_height() + offset,
                       f'{val:.2f}M',
                       ha='center', va='bottom', fontsize=9)

        annotate(bars_total, totals_M)
        annotate(bars_train, trains_M)

        plt.tight_layout()
        plt.close(fig)
        return fig

    def plot_training_losses(self, results_csv_path):
        """
        Plot train vs validation losses (box, cls, dfl) from Ultralytics results.
        
        Args:
            results_csv_path: Path to the results.csv file generated by Ultralytics
        """
        results_csv_path = Path(results_csv_path)
        if not results_csv_path.exists():
            print(f"Results CSV not found at {results_csv_path}")
            return None
        
        df = pd.read_csv(results_csv_path)
        df.columns = df.columns.str.strip()
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training vs Validation Losses', fontsize=16, fontweight='bold')
        
        epochs = df['epoch'] if 'epoch' in df.columns else range(len(df))
        
        # Plot 1: Box Loss
        ax1 = axes[0, 0]
        if 'train/box_loss' in df.columns and 'val/box_loss' in df.columns:
            ax1.plot(epochs, df['train/box_loss'], label='Train Box Loss', 
                    color='#e74c3c', linewidth=2, marker='o', markersize=4)
            ax1.plot(epochs, df['val/box_loss'], label='Val Box Loss', 
                    color='#3498db', linewidth=2, marker='s', markersize=4)
            ax1.set_xlabel('Epoch', fontsize=11)
            ax1.set_ylabel('Box Loss', fontsize=11)
            ax1.set_title('Bounding Box Loss', fontsize=12, fontweight='bold')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # Plot 2: Class Loss
        ax2 = axes[0, 1]
        if 'train/cls_loss' in df.columns and 'val/cls_loss' in df.columns:
            ax2.plot(epochs, df['train/cls_loss'], label='Train Class Loss', 
                    color='#e74c3c', linewidth=2, marker='o', markersize=4)
            ax2.plot(epochs, df['val/cls_loss'], label='Val Class Loss', 
                    color='#3498db', linewidth=2, marker='s', markersize=4)
            ax2.set_xlabel('Epoch', fontsize=11)
            ax2.set_ylabel('Class Loss', fontsize=11)
            ax2.set_title('Classification Loss', fontsize=12, fontweight='bold')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        # Plot 3: DFL Loss
        ax3 = axes[1, 0]
        if 'train/dfl_loss' in df.columns and 'val/dfl_loss' in df.columns:
            ax3.plot(epochs, df['train/dfl_loss'], label='Train DFL Loss', 
                    color='#e74c3c', linewidth=2, marker='o', markersize=4)
            ax3.plot(epochs, df['val/dfl_loss'], label='Val DFL Loss', 
                    color='#3498db', linewidth=2, marker='s', markersize=4)
            ax3.set_xlabel('Epoch', fontsize=11)
            ax3.set_ylabel('DFL Loss', fontsize=11)
            ax3.set_title('Distribution Focal Loss', fontsize=12, fontweight='bold')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # Plot 4: Combined Total Loss
        ax4 = axes[1, 1]
        train_total = np.zeros(len(df))
        val_total = np.zeros(len(df))
        
        if 'train/box_loss' in df.columns:
            train_total += df['train/box_loss'].fillna(0)
        if 'train/cls_loss' in df.columns:
            train_total += df['train/cls_loss'].fillna(0)
        if 'train/dfl_loss' in df.columns:
            train_total += df['train/dfl_loss'].fillna(0)
            
        if 'val/box_loss' in df.columns:
            val_total += df['val/box_loss'].fillna(0)
        if 'val/cls_loss' in df.columns:
            val_total += df['val/cls_loss'].fillna(0)
        if 'val/dfl_loss' in df.columns:
            val_total += df['val/dfl_loss'].fillna(0)
        
        ax4.plot(epochs, train_total, label='Train Total Loss', 
                color='#e74c3c', linewidth=2, marker='o', markersize=4)
        ax4.plot(epochs, val_total, label='Val Total Loss', 
                color='#3498db', linewidth=2, marker='s', markersize=4)
        ax4.set_xlabel('Epoch', fontsize=11)
        ax4.set_ylabel('Total Loss', fontsize=11)
        ax4.set_title('Combined Loss', fontsize=12, fontweight='bold')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.close(fig)
        return fig

    def plot_metrics(self, results_csv_path):
        """
        Plot mAP metrics and precision/recall from Ultralytics results.
        
        Args:
            results_csv_path: Path to the results.csv file generated by Ultralytics
        """
        results_csv_path = Path(results_csv_path)
        if not results_csv_path.exists():
            print(f"Results CSV not found at {results_csv_path}")
            return None
        
        df = pd.read_csv(results_csv_path)
        df.columns = df.columns.str.strip()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        fig.suptitle('Validation Metrics', fontsize=16, fontweight='bold')
        
        epochs = df['epoch'] if 'epoch' in df.columns else range(len(df))
        
        # Plot 1: Precision and Recall
        ax1 = axes[0]
        if 'metrics/precision(B)' in df.columns:
            ax1.plot(epochs, df['metrics/precision(B)'], label='Precision', 
                    color='#2ecc71', linewidth=2, marker='o', markersize=4)
        if 'metrics/recall(B)' in df.columns:
            ax1.plot(epochs, df['metrics/recall(B)'], label='Recall', 
                    color='#9b59b6', linewidth=2, marker='s', markersize=4)
        ax1.set_xlabel('Epoch', fontsize=11)
        ax1.set_ylabel('Score', fontsize=11)
        ax1.set_title('Precision & Recall', fontsize=12, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim([0, 1.05])
        
        # Plot 2: mAP Scores
        ax2 = axes[1]
        if 'metrics/mAP50(B)' in df.columns:
            ax2.plot(epochs, df['metrics/mAP50(B)'], label='mAP@0.5', 
                    color='#e67e22', linewidth=2, marker='o', markersize=4)
        if 'metrics/mAP50-95(B)' in df.columns:
            ax2.plot(epochs, df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', 
                    color='#1abc9c', linewidth=2, marker='s', markersize=4)
        ax2.set_xlabel('Epoch', fontsize=11)
        ax2.set_ylabel('mAP Score', fontsize=11)
        ax2.set_title('Mean Average Precision', fontsize=12, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim([0, 1.05])
        
        plt.tight_layout()
        plt.close(fig)
        return fig

    def export_model_summary_csv(self, trainer, output_path=None):
        """
        Export model architecture summary to CSV.
        
        Args:
            trainer: Ultralytics trainer object
            output_path: Path to save CSV (optional, auto-generates if None)
        
        Returns:
            Path to the saved CSV file
        """
        if output_path is None:
            save_dir = Path(trainer.save_dir)
            output_path = save_dir / 'model_summary.csv'
        
        output_path = Path(output_path)
        
        # Get model info
        model = trainer.model
        
        # Collect layer information
        layer_data = []
        
        for idx, (name, module) in enumerate(model.named_modules()):
            if len(list(module.children())) == 0:  # Only leaf modules
                num_params = sum(p.numel() for p in module.parameters())
                trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
                
                layer_data.append({
                    'Layer_Index': idx,
                    'Layer_Name': name,
                    'Layer_Type': module.__class__.__name__,
                    'Total_Parameters': num_params,
                    'Trainable_Parameters': trainable_params,
                    'Frozen_Parameters': num_params - trainable_params,
                    'Is_Trainable': trainable_params > 0
                })
        
        # Create DataFrame and save
        df = pd.DataFrame(layer_data)
        df.to_csv(output_path, index=False)
        
        print(f"Model summary saved to: {output_path}")
        return output_path

    def export_class_metrics_csv(self, metrics_output, output_path=None):
        """
        Export per-class validation metrics to CSV from model.val() output.
        
        Args:
            metrics_output: Output from model.val() containing per-class metrics
            output_path: Path to save CSV (optional)
        
        Returns:
            Path to the saved CSV file
        """
        if output_path is None:
            output_path = Path.cwd() / 'class_metrics.csv'
        
        output_path = Path(output_path)
        
        # Extract metrics - Ultralytics stores them in different attributes
        # Check if metrics has the data we need
        if hasattr(metrics_output, 'box'):
            box_metrics = metrics_output.box
            
            # Get class names
            class_names = metrics_output.names if hasattr(metrics_output, 'names') else {}
            
            # Build data
            class_data = []
            
            # Per-class metrics are in box.ap_class_index and corresponding arrays
            if hasattr(box_metrics, 'ap_class_index'):
                for idx, class_idx in enumerate(box_metrics.ap_class_index):
                    class_name = class_names.get(int(class_idx), f"class_{class_idx}")
                    
                    class_data.append({
                        'Class_Index': int(class_idx),
                        'Class_Name': class_name,
                        'Precision': float(box_metrics.p[idx]) if hasattr(box_metrics, 'p') and idx < len(box_metrics.p) else None,
                        'Recall': float(box_metrics.r[idx]) if hasattr(box_metrics, 'r') and idx < len(box_metrics.r) else None,
                        'mAP50': float(box_metrics.ap50[idx]) if hasattr(box_metrics, 'ap50') and idx < len(box_metrics.ap50) else None,
                        'mAP50-95': float(box_metrics.ap[idx]) if hasattr(box_metrics, 'ap') and idx < len(box_metrics.ap) else None,
                        'F1_Score': float(2 * (box_metrics.p[idx] * box_metrics.r[idx]) / (box_metrics.p[idx] + box_metrics.r[idx] + 1e-6)) if hasattr(box_metrics, 'p') and hasattr(box_metrics, 'r') else None
                    })
            
            # Create DataFrame and save
            if class_data:
                df = pd.DataFrame(class_data)
                df.to_csv(output_path, index=False)
                print(f"Class metrics saved to: {output_path}")
                return output_path
            else:
                print("No per-class metrics found to export")
                return None
        else:
            print("Metrics object doesn't contain box metrics")
            return None

    def export_confusion_matrix_analysis(self, trainer, output_path=None):
        """
        Export confusion matrix analysis including misclassifications to CSV.
        
        Args:
            trainer: Ultralytics trainer object
            output_path: Path to save CSV (optional)
        
        Returns:
            Path to the saved CSV file
        """
        if output_path is None:
            save_dir = Path(trainer.save_dir)
            output_path = save_dir / 'misclassifications.csv'
        
        output_path = Path(output_path)
        
        # Get validation results
        if hasattr(trainer, 'validator') and trainer.validator is not None:
            validator = trainer.validator
            
            # Get confusion matrix
            if hasattr(validator, 'confusion_matrix') and validator.confusion_matrix is not None:
                cm = validator.confusion_matrix.matrix
                class_names = list(validator.names.values()) if hasattr(validator, 'names') else []
                
                # Analyze misclassifications
                misclass_data = []
                
                for true_idx in range(len(cm)):
                    for pred_idx in range(len(cm[true_idx])):
                        count = int(cm[true_idx][pred_idx])
                        
                        if count > 0:
                            true_class = class_names[true_idx] if true_idx < len(class_names) else f"class_{true_idx}"
                            pred_class = class_names[pred_idx] if pred_idx < len(class_names) else f"class_{pred_idx}"
                            
                            is_correct = true_idx == pred_idx
                            
                            misclass_data.append({
                                'True_Class_Index': true_idx,
                                'True_Class_Name': true_class,
                                'Predicted_Class_Index': pred_idx,
                                'Predicted_Class_Name': pred_class,
                                'Count': count,
                                'Is_Correct': is_correct,
                                'Error_Type': 'Correct' if is_correct else 'Misclassification'
                            })
                
                # Create DataFrame and save
                df = pd.DataFrame(misclass_data)
                
                # Sort by count descending to highlight most common errors
                df = df.sort_values('Count', ascending=False)
                
                df.to_csv(output_path, index=False)
                print(f"Confusion matrix analysis saved to: {output_path}")
                return output_path
            else:
                print("Confusion matrix not available")
                return None
        else:
            print("Validator not available")
            return None

    def log_all_training_visualizations(self, trainer, total_params=None, trainable_params=None):
        """
        Comprehensive logging of all training visualizations and CSVs to W&B.
        This should be called in the on_train_end callback.
        
        Args:
            trainer: Ultralytics trainer object with access to results
            total_params: Total parameter count (optional, will calculate if not provided)
            trainable_params: Trainable parameter count (optional, will calculate if not provided)
        """
        save_dir = Path(trainer.save_dir)
        results_csv = save_dir / 'results.csv'
        
        visualizations = {}
        
        # 1. Log trainable parameters plot (if params provided)
        if total_params is not None and trainable_params is not None:
            try:
                param_fig = self.plot_trainable_parameters(total_params, trainable_params)
                visualizations['model/trainable_parameters'] = wandb.Image(param_fig)
                print(f"✓ Logged trainable parameters plot")
            except Exception as e:
                print(f"✗ Error logging trainable parameters: {e}")
        
        # 2. Plot and log training losses
        try:
            loss_fig = self.plot_training_losses(results_csv)
            if loss_fig:
                visualizations['training/losses'] = wandb.Image(loss_fig)
                print(f"✓ Logged training losses")
        except Exception as e:
            print(f"✗ Error logging losses: {e}")
        
        # 3. Plot and log metrics
        try:
            metrics_fig = self.plot_metrics(results_csv)
            if metrics_fig:
                visualizations['validation/metrics'] = wandb.Image(metrics_fig)
                print(f"✓ Logged validation metrics")
        except Exception as e:
            print(f"✗ Error logging metrics: {e}")
        
        # 4. Export and log model summary CSV
        try:
            model_summary_path = self.export_model_summary_csv(trainer)
            if model_summary_path and model_summary_path.exists():
                wandb.log({"model/architecture_summary": wandb.Table(dataframe=pd.read_csv(model_summary_path))})
                print(f"✓ Logged model summary CSV")
        except Exception as e:
            print(f"✗ Error exporting model summary: {e}")
        
        # 5. Export and log confusion matrix analysis
        try:
            misclass_path = self.export_confusion_matrix_analysis(trainer)
            if misclass_path and misclass_path.exists():
                wandb.log({"validation/misclassifications": wandb.Table(dataframe=pd.read_csv(misclass_path))})
                print(f"✓ Logged misclassifications CSV")
        except Exception as e:
            print(f"✗ Error exporting confusion matrix: {e}")
        
        # 6. Log Ultralytics' built-in plots
        plots_to_log = {
            'confusion_matrix': save_dir / 'confusion_matrix.png',
            'confusion_matrix_normalized': save_dir / 'confusion_matrix_normalized.png',
            'results': save_dir / 'results.png',
            'PR_curve': save_dir / 'PR_curve.png',
            'F1_curve': save_dir / 'F1_curve.png',
            'P_curve': save_dir / 'P_curve.png',
            'R_curve': save_dir / 'R_curve.png',
            'labels': save_dir / 'labels.jpg',
            'labels_correlogram': save_dir / 'labels_correlogram.jpg',
        }
        
        for plot_name, plot_path in plots_to_log.items():
            if plot_path.exists():
                visualizations[f'ultralytics/{plot_name}'] = wandb.Image(str(plot_path))
        
        # 7. Log validation batch predictions
        val_batch_path = save_dir / 'val_batch0_pred.jpg'
        if val_batch_path.exists():
            visualizations['validation/predictions'] = wandb.Image(str(val_batch_path))
        
        # Log everything to W&B
        if visualizations:
            wandb.log(visualizations)
            print(f"\n✓ Logged {len(visualizations)} visualizations to W&B")
        
        return visualizations